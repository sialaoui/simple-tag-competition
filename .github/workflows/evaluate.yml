name: Evaluate Student Submission

on:
  pull_request_target:   
    types: [opened, synchronize, reopened]
    paths: 
      - 'submissions/**'
      
permissions:
  contents: write
  pull-requests: write
  pages: write
  id-token: write
  actions: write

jobs:
  evaluate:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout main branch (leaderboard + history)
        uses: actions/checkout@v4
        with: 
          ref: main
          fetch-depth: 0
          persist-credentials: true

      - name: Checkout submission (PR head)
        uses: actions/checkout@v4
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          path: submission_repo
          fetch-depth: 1
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Locate submission directory
        id: submission
        run: |
          # Find exactly one submission folder inside the student's PR checkout
          submission_root="submission_repo/submissions"

          if [ ! -d "$submission_root" ]; then
            echo "Error: submissions directory not found in PR checkout"
            exit 1
          fi

          # Expect exactly one student folder with agent.py
          submission_dirs=$(find "$submission_root" -maxdepth 1 -mindepth 1 -type d)

          count=$(echo "$submission_dirs" | sed '/^$/d' | wc -l | tr -d ' ')

          if [ "$count" -eq 0 ]; then
            echo "Error: No submission folder found under submissions/"
            exit 1
          fi

          if [ "$count" -gt 1 ]; then
            echo "Error: Multiple submission folders found under submissions/. Please submit only one."
            echo "$submission_dirs"
            exit 1
          fi

          submission_dir="$submission_dirs"

          username=$(basename "$submission_dir")

          echo "username=$username" >> $GITHUB_OUTPUT
          echo "submission_dir=$submission_dir" >> $GITHUB_OUTPUT
          echo "Found submission from: $username"
      
      - name: Validate submission structure
        run: |
          submission_dir="${{ steps.submission.outputs.submission_dir }}"
          
          # Check if agent.py exists
          if [ ! -f "$submission_dir/agent.py" ]; then
            echo "Error: agent.py not found in $submission_dir"
            exit 1
          fi
          
          # Check for only allowed files
          for file in "$submission_dir"/*; do
            filename=$(basename "$file")
            if [[ ! "$filename" =~ ^(agent\.py|.*\.pth)$ ]]; then
              echo "Error: Invalid file found: $filename"
              echo "Only agent.py and .pth files are allowed"
              exit 1
            fi
          done
          
          echo "  Submission structure is valid"
      
      - name: Setup reference prey agent
        run: |
          mkdir -p reference_agents
          cp reference_agents_source/prey_agent.py reference_agents/prey_agent.py
          cp reference_agents_source/prey_model.pth reference_agents/prey_model.pth
      
      - name: Run evaluation
        id: evaluate
        run: |
          submission_dir="${{ steps.submission.outputs.submission_dir }}"
          username="${{ steps.submission.outputs.username }}"
          
          # Create results directory (preserves existing leaderboard files from main checkout)
          mkdir -p results
          
          # Run evaluation
          output_file="results/${username}_$(date +%s).json"
          
          python evaluate.py \
            --submission-dir "$submission_dir" \
            --reference-agents-dir "reference_agents" \
            --output "$output_file" \
            --episodes 500
          
          echo "output_file=$output_file" >> $GITHUB_OUTPUT
      
      - name: Generate leaderboard
        run: |
          python generate_leaderboard.py \
            --results-dir results \
            --output docs/index.html
      
      - name: Extract scores for PR comment
        id: scores
        run: |
          output_file="${{ steps.evaluate.outputs.output_file }}"
          
          if [ -f "$output_file" ]; then
            predator_score=$(jq -r '.predator_score' "$output_file")
            
            echo "predator_score=$predator_score" >> $GITHUB_OUTPUT
          fi
      
      - name: Commit and push results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          git add results/
          git add docs/index.html
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Add evaluation results for ${{ steps.submission.outputs.username }}"
            git push origin main
          fi
      
      - name: Setup Pages
        uses: actions/configure-pages@v4
      
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'docs'
      
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
      
      - name: Comment on PR with results
        uses: actions/github-script@v7
        with:
          script: |
            const username = '${{ steps.submission.outputs.username }}';
            const predatorScore = '${{ steps.scores.outputs.predator_score }}';
            
            const body = `## ðŸŽ¯ Evaluation Results
            
            **Submission by:** @${username}
            
            ### Scores
            
            ${parseFloat(predatorScore).toFixed(4)}
            
            ### Details
            - **Episodes:** 500
            - **Environment:** Simple Tag (PettingZoo MPE)
            - **Metric:** Average predator reward vs reference prey
            
            ### Next Steps
            
            - Your submission has been evaluated and added to the leaderboard!
            
            - Check the [live leaderboard](https://${context.repo.owner}.github.io/${context.repo.repo}/) to see your ranking.
            
            - Want to improve your score? Submit a new PR with an updated agent!
            
            **Note:** This PR will not be merged. Your score has been recorded on the main branch automatically.
            
            ---
            *Evaluation: 500 episodes against public reference prey with deterministic seeding.*
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
